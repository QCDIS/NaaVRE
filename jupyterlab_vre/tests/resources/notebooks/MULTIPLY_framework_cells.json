{
  "save": false,
  "cell_index": 0,
  "notebook": {
    "metadata": {
      "kernelspec": {
        "display_name": "Python 3 (ipykernel)",
        "language": "python",
        "name": "python3"
      },
      "language_info": {
        "codemirror_mode": {
          "name": "ipython",
          "version": 3
        },
        "file_extension": ".py",
        "mimetype": "text/x-python",
        "name": "python",
        "nbconvert_exporter": "python",
        "pygments_lexer": "ipython3",
        "version": "3.9.13"
      }
    },
    "nbformat_minor": 4,
    "nbformat": 4,
    "cells": [
      {
        "cell_type": "markdown",
        "source": "# 1\tIntroduction\nThe MULTIPLY framework is a new toolkit to extract information from remote sensing data. In contrast to other remote sensing software packages, MULTIPLY is designed to consistently use heterogeneous satellite data (coarse vs high, optical vs microwave). Instead of multiple independent process-chains for each individual satellite, MULTIPLY is a single all-in-coupling framework in order to:\n*\tCreate consistencies between different land surface products (reducing errors in downstream-processing)\n*\tUtilize the benefits of multiple satellites (including Sentinel-1, Sentinel-2, MODIS, Landsat), such as \n    * Higher temporal coverage\n    * Spectral Sensitivity to more land surface variables\n*\tQuantify errors of output products, depending on input uncertainties.\n*\tEnable operational service for different end-users, including: \n    * Remote sensing consultants\n    *\tRemote sensing experts\nIn order to achieve these goals MULTIPLY uses a data-assimilation framework developed for the ESA (Gómez-Dans et al. 2016; Lewis et al. 2012). In this framework the state-vector (describing the land surface parameters of interest) are coupled to the observations using radiative transfer models. In addition priori-information (obtained from field-measurements databases) are used to constrain the final results. \n",
        "metadata": {}
      },
      {
        "cell_type": "markdown",
        "source": "# 2\tMULTIPLY Concepts\n## 2.1\tRadiative transfer models\nIn it’s most basic description, a radiative transfer model is a model  capable of (using a specific state-vector) as an input simulateing the different interactions of radiation (light) from source (sun/clouds/..) to sensor (satellite ), using a specific state-vector as an input.  Within the model the most important absorption/transmission and reflection processes by different objects (soil/leaf/atmosphere) is characterized. In this regard there exist many radiative transfer models exist, depending on the level and detail of which important processes are taken into account. In practice, highly complex radiative radiative transfer models usually have a high number of input parameters (to describe in detail the 3D structure of the land surface and the full land surface heterogeneity), while less complex models employ specific hypotheses (such as land surface homogeneity) to require less input parameters. \n\n<img src=\"pics/BasicDescriptionRTM2.png\">\nFigure 1: Basic description of Radiative Transfer model",
        "metadata": {}
      },
      {
        "cell_type": "markdown",
        "source": "While this produces a large variety in radiative transfer models, all of these models have at least one common aspect: they can only be run in ‘forward mode’. This is because completely different scenarios can produce similar remote sensing observations. For example, when a land surface has a very low fractional vegetation cover (~0), different chlorophyll levels will produce similar remote sensing observations.",
        "metadata": {}
      },
      {
        "cell_type": "markdown",
        "source": "## 2.2\tData assimilation\nThe most common way to circumvent the shortcoming of radiative transfer models is to ‘guess’ the initial state-vector, run the radiative transfer model using this initial assumption, and compare the output observation against the remote sensing measurement, producing an error. By guessing multiple times,  (for instance by constructing a Look-up-Table (LUT), multiple error-values can be calculated. The lowest error-value than represents our best understanding of the land surface (as long as our initial assumption was close enough to the real land surface description). \n\n<img src=\"pics/CreatingLUTs.png\">\nFigure 2: Creating LUT tables\n\nSuch an LUT approach has the large disadvantage that it only finds values that were used in the initial guessing; for example if for the Leaf Area Index (LAI) only values [0, 1, 2,.. ,6] were used, this retrieval method will not find the (actual) value of 1.22 m2/m2. \nThe shortcoming of the LUT approach can be greatly to a large part be circumvented if the initial scenarios can be updated on basis of the errors found,  (together with taking into account the sensitivity of the model to the state vector), as illustrated by in Figure 3. Such an iterative approach is in general ‘optimization’ of the radiative transfer model. \n \n<img src=\"pics/BasicOptimization.png\">\nFigure 3: Basic description of Optimization methodology\n\nIn this approach, it is possible for the state-vector to ‘slowly’ drift away from our prior-value. In addition, the approach is limited to produce only results for when satellite measurements are being performed. In order to solve these shortcomings additional (prior) information can be used within a data-assimilation framework. This is illustrated by Figure 4.\n\n<img src=\"pics/DataAssimilation.png\">\nFigure 4: Data Assimilation of Radiative Transfer Models",
        "metadata": {}
      },
      {
        "cell_type": "markdown",
        "source": "## 2.3\tPrior Information\nIn such an data-assimilation approach, an additional error (Error value 2) different from the observational error (Error 1) is defined, describing the deviation of the state-vector from our initial assumption. Here the observational error is defined in units of the observation (e.q. reflectances [-] or radiances [W sr^(-1) m^(-2) nm^(-1)]), while the prior-error is defined in units of the state vector (LAI  [m^2  m^(-2)], Leaf Angle Distribition [deg], …). In order to reconcile these two into one cost function the errors are normalized. For the observational error this is performed by considering the observational errors, while for the prior information this is accomplished by considering the prior uncertainty (for example the standard deviation of long-term measurements of the respective land surface parameter). This leads then to one single cost-function defined as:\n\nJ_tot=(Error_1)/σ_1 +  (Error_2)/σ_2 =J_obs+J_prior\n\nThis also immediately provides us with a method on synergistically combining different types of observations, as the observational error can simply be expanded to include multiple observations\n\nJ_obs=J_obs^1+ J_obs^2+ J_obs^3+⋯\n\nIn short, each of the error-terms corresponds to different pieces of information added to the system. In this case, even more additional information can also be introduced to the system, be considering the temporal evolution and spatial variation of specific land surface parameters. \n",
        "metadata": {}
      },
      {
        "cell_type": "markdown",
        "source": "# 3\tThe prototype version overview \nWe hereby provide you with a working prototype  version. This prototype  version is a stripped down version of the actual framework focusing primarily on the optical retrieval using both coarse (MODIS) and high resolution (Sentinel-2) observations. An overview of the full framework is provided in Figure 6. Here the specific modules that were not mature to be incorporated in this release, are the SAR preprocessing, SAR integration, Post processing (for biodiversity and fire disturbance monitoring), as well as the visualization component (highlighted by red circles )\n\n<img src=\"pics/MultiplyModules.png\">\nFigure 5: Overview of MULTIPLY framework. Modules highlighted by a red circle are not included in the prototype version",
        "metadata": {}
      },
      {
        "cell_type": "markdown",
        "source": "In the version you will be able to download al the required earth observation data (using the data-access component), and afterwards retrieve land surface parameters (using the Inference Engine Module) at  20m resolution using a combination of MODIS coarse resolution data (from the Coarse res Preprocessing module) together with high resolution Sentinel-2 observations (from the high res preprocessing module), together with prior information on the land surface (by the Prior Engine module). In the future it is foreseen that multiple Radiative transfer models can be used, specific to the user requirements, however at the moment the only one available (in the forward operator module) is the emulated version of the PROSAIL radiative transfer mode . This limits the retrieval of land surface parameters to a) leaf-traits (Chlorophyll, Carotonoids, Dry matter, Water Content),  b) Vegetation structural parameters (hotspot parameter, LAI, leaf structure), and c some auxiliary soil parameters (Bsoil/Psoil).\n\n<img src=\"pics/MultiplyOutput.png\">\nFigure 6: Overview of state-vector parameters retrievable on basis of the underlying forward operator (radiative transfer model",
        "metadata": {}
      },
      {
        "cell_type": "markdown",
        "source": "# 4 Usage\nAll the code for the individual modules are is located at https://github.com/multiply-org/. This can be used to setup the MULTIPLY framework on your own computing infrastructure. At present however no deployment setup (in the form of windows-setup-executables, or anaconda package’s) exist. While this is planned further intolater in the project, the focus at this stage is on testing the individual components themselves. Please let us know if you would prefer to install the software yourself on a dedicated computational framework, so that we can investigate how to facilitate this for you. \nIn order to facilitate the testing of the framework itself, we have setup this Virtual Machine on Google Compute Engine, for testing purposes. \n",
        "metadata": {}
      },
      {
        "cell_type": "markdown",
        "source": "## 4.0 Load internal packages and auxiliary methods",
        "metadata": {}
      },
      {
        "cell_type": "code",
        "source": "import os\nfrom multiply_data_access import DataAccessComponent\nfrom multiply_orchestration import create_sym_links\nimport datetime\nimport glob\nfrom typing import List, Optional\nfrom vm_support import create_config_file, set_permissions\nfrom multiply_prior_engine import PriorEngine\n",
        "metadata": {},
        "execution_count": 1,
        "outputs": [
          {
            "name": "stderr",
            "output_type": "stream",
            "text": "/home/jovyan/data-access/multiply_data_access/local_file_system.py:64: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if pattern is '':\n/home/jovyan/data-access/multiply_data_access/local_file_system.py:204: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n  if data_type is not '':\n/home/jovyan/data-access/multiply_data_access/data_access_component.py:144: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if data_type is '':\n/home/jovyan/prior-engine/multiply_prior_engine/vegetation_prior_creator.py:1128: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  if varname is 'cdm':\nINFO:root:The config file can be found at /home/jovyan/prior-engine/multiply_prior_engine/prior_engine_logging.yml\n"
          },
          {
            "name": "stdout",
            "output_type": "stream",
            "text": "INFO     prior_logger __init__:  69: ------------- Logger initialized. -------------\nINFO     prior_logger __init__:  70: The log file can be found at /tmp/MULTIPLYPriorEngine/prior_engine.log\nINFO     __init__ <module>:  30: The temporary directory is set to /tmp/MULTIPLYPriorEngine\n"
          }
        ]
      },
      {
        "cell_type": "markdown",
        "source": "## 4.1 Defining the interfaces to work with the MULTIPLY framework\nFirst we need to setup interfaces to work and test with the MULTIPLY framework. In actual operation, these auxiliary interfaces are bundled together within a single python class. However as we would like you to test the different modules individually, we provide direct access to these. For more information on how these interfaces are defined, please check the following link: [MULTIPLY Tools](Tools.py). \n",
        "metadata": {}
      },
      {
        "cell_type": "code",
        "source": "from vm_support.tools import get_static_data, get_dynamic_data",
        "metadata": {},
        "execution_count": 2,
        "outputs": [
          {
            "ename": "ModuleNotFoundError",
            "evalue": "No module named 'Tools'",
            "output_type": "error",
            "traceback": [
              "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
              "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
              "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_static_data, get_dynamic_data\n",
              "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Tools'"
            ]
          }
        ]
      },
      {
        "cell_type": "code",
        "source": "from Tools import preprocess",
        "metadata": {},
        "execution_count": 3,
        "outputs": []
      },
      {
        "cell_type": "code",
        "source": "from Tools import get_priors, get_priors_from_config_file",
        "metadata": {},
        "execution_count": 4,
        "outputs": []
      },
      {
        "cell_type": "code",
        "source": "from Tools import infer, infer3, infer_new",
        "metadata": {},
        "execution_count": 5,
        "outputs": []
      },
      {
        "cell_type": "code",
        "source": "from Tools import create_dir, put_data, InvTransformation",
        "metadata": {},
        "execution_count": 6,
        "outputs": []
      },
      {
        "cell_type": "code",
        "source": "from Tools import Plot_SRDS,Plot_PRIORS, Plot_TRAITS,Plot_TRAIT_evolution, Plot_Transformation",
        "metadata": {},
        "execution_count": 7,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "source": "# 5 Running MULTIPLY\nBelow the actual code is provided for running the MULTIPLY framework.\nWe start with setting earth data authentication. This is required to download the MODIS brdf descriptors which are required for the atmospheric correction of the Sentinel-2 data. You can get credentials when you register at https://urs.earthdata.nasa.gov/profile . Registration and use is free of cost. If you do not register, you can only use the MODIS data which has been downloaded in previous runs of the notebook by other users.\nAlso you will need to set up the data stores so that the data access component is working correctly and finds the pre-configured data stores. Both steps only need to be performed once.",
        "metadata": {}
      },
      {
        "cell_type": "code",
        "source": "from vm_support import set_earth_data_authentication, set_up_data_stores\nset_up_data_stores()\nusername = 'Multiply' \npassword = 'XXXXXXXXXXXXXX'\nset_earth_data_authentication(username, password) # to download modis data, needs only be done once",
        "metadata": {},
        "execution_count": 8,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "source": "## 5.1 Parameters\nHere you can actually set the parameters for the run. The parameters are as follows:\n* **roi**: A region of interest, given as a Polygon in WKT format. You can use this tool ( https://arthur-e.github.io/Wicket/sandbox-gmaps3.html ) to easily get definitions of the regions you are interested in in WGS84 coordinates.\n* **roi_grid**: The EPSG-code of the spatial reference system in which the roi is given. If it is set to 'none', it is assumed that the roi is given in WGS84 coordinates.\n* **destination_grid**: The EPSG-code of the spatial reference system in which the output shall be given. If it is set to 'none', the platform will attempt to derive it from the roi_grid.\n* **spatial_resolution**: The resolution the output data is supposed to have, must be a non-negative integer number. The resolution is given in meters and is the same for both dimensions.\n* **start_time**: The start date of the period you are interested in, must be given in the format 'Year-Month-Day' as below.\n* **end_time**: The end date of the period you are interested in, must be given in the format 'Year-Month-Day' as below.\n* **time_step**: The temporal resolution the output is supposed to have. Data will be aggregated over the period denoted by this parameter. Must be a non-negative integer value. The unit is days.\n* **variables**: The list of the biophysical variables that shall be derived. Please do not change this list, as the underlying forward model requires all of them. The parameters are as follows:\n  * **n**: Structural parameter\n  * **cab**: Leaf Chlorophyll Content, given in ug/cm²\n  * **car**: Leaf Carotonoid Content, given in ug/cm²\n  * **cb**: Leaf senescent material\n  * **cw**: Leaf Water Content, given in cm\n  * **cdm**: Leaf Dry Mass, given in g/cm²\n  * **lai**: Effective Leaf Area Index, given in m²/m²\n  * **ala**: Average Leaf Angle, given in degrees\n  * **bsoil**: Soil Brightness Parameter\n  * **psoil**: Soil Wetness Parameter\n* **file_mask**: A file that can be used to explicitly state the region you are interested in. You can also use it to mask out single pixels within this region. If this is not 'none', the aforementioned parameters roi_grid, spatial_resolution, and destination_grid are not used.\n\n**HINT**: The platform will perform faster the smaller your roi and the larger the spatial resolution is.",
        "metadata": {}
      },
      {
        "cell_type": "markdown",
        "source": "### 5.1.1 Define region of Interest",
        "metadata": {}
      },
      {
        "cell_type": "code",
        "source": "roi_grid = 'EPSG:4326'\nroi_centroid = [27.3, 58.3]\nroi = 'POLYGON((27.1615456795 58.2252523466, ' + \\\n                '27.1720041127 58.3418423196, ' + \\\n                '27.3885799673 58.3362682904, ' + \\\n                '27.3774098023 58.2196966529, ' + \\\n                '27.1615456795 58.2252523466))'\n\ndestination_grid = 'EPSG:3301'                \nspatial_resolution = 20 # in m",
        "metadata": {},
        "execution_count": 9,
        "outputs": []
      },
      {
        "cell_type": "code",
        "source": "from IPython.display import IFrame\nfrom IPython.core.display import display\n\ngoogle_maps_url = \"http://maps.google.com/maps?q=48.184543+11.213&ie=UTF8&t=h&z=18&output=embed&z=17\"\ngoogle_maps_url = 'http://maps.google.com/maps?q=%7.5f' % roi_centroid[1] + '+%7.5f' % roi_centroid[0] +'&ie=UTF8&t=h&z=18&output=embed&z=10'\nIFrame(google_maps_url,800,600)\n\n",
        "metadata": {},
        "execution_count": 11,
        "outputs": [
          {
            "execution_count": 11,
            "output_type": "execute_result",
            "data": {
              "text/html": "\n        <iframe\n            width=\"800\"\n            height=\"600\"\n            src=\"http://maps.google.com/maps?q=58.30000+27.30000&ie=UTF8&t=h&z=18&output=embed&z=10\"\n            frameborder=\"0\"\n            allowfullscreen\n        ></iframe>\n        ",
              "text/plain": "<IPython.lib.display.IFrame at 0x7f9deb32a588>"
            },
            "metadata": {}
          }
        ]
      },
      {
        "cell_type": "markdown",
        "source": "\n### 5.1.2 Define temporal frequency and period",
        "metadata": {}
      },
      {
        "cell_type": "code",
        "source": "# start_time_as_string = '2018-05-8'\n# stop_time_as_string = '2018-05-9'\n\nstart_time_as_string = '2018-07-24'\nstop_time_as_string = '2018-07-25'\n\n\n# start_time_as_string = '2018-05-13'\n# stop_time_as_string = '2018-05-14'\n\ntime_step = 1 # in days\n\nstart_time_as_datetime = datetime.datetime.strptime(start_time_as_string, '%Y-%m-%d')\nstop_time_as_datetime = datetime.datetime.strptime(stop_time_as_string, '%Y-%m-%d')\n\ntime_step_as_time_delta = datetime.timedelta(days=time_step)\nvariables = {'n', 'cab', 'car', 'cb', 'cw', 'cdm', 'lai', 'ala', 'bsoil', 'psoil'}\n\n# file_mask = \"mask.tif\"\nfile_mask = None",
        "metadata": {},
        "execution_count": 12,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "source": "### 5.1.3 Setting up the working directory\nFor this notebook, you will operate in your own working directory. All data you use will be copied here, all output will be written here.",
        "metadata": {}
      },
      {
        "cell_type": "code",
        "source": "from vm_support import get_working_dir\nname = 'm1'\n# use previous (non-empty) working directory\n# working_dir = '/Data/test_user_##/' + name\n\n#clear working directory\nworking_dir = get_working_dir(name)\n\n",
        "metadata": {},
        "execution_count": 13,
        "outputs": []
      },
      {
        "cell_type": "code",
        "source": "print('Working directory is {}'.format(working_dir))\n\npriors_directory = '{}/priors'.format(working_dir)\nhres_state_dir = '{}/hresstate'.format(working_dir)\nmodis_directory = '{}/modis'.format(working_dir)\nstate_directory = '{}/state'.format(working_dir)\ncams_directory = '{}/cams'.format(working_dir)\ns2_l1c_directory = '{}/s2'.format(working_dir)\nsdrs_directory = '{}/sdrs'.format(working_dir)\nbiophys_output = '{}/biophys'.format(working_dir)\nemulators_directory = '{}/emulators'.format(working_dir)\ndem_directory = '{}/dem'.format(working_dir)",
        "metadata": {},
        "execution_count": 14,
        "outputs": [
          {
            "name": "stdout",
            "output_type": "stream",
            "text": "Working directory is /data/test_user_45/m1\n"
          }
        ]
      },
      {
        "cell_type": "markdown",
        "source": "## 5.2 Acquire Static Data\nWe differentiate between two types of data here: Dynamic and static, meaning: Data which are valid for a certain period of time and data which is valid permanently. The latter is the elevation data of the Digital Elevation Model and the Emulators required for the Atmospheric Correction. We put them in their designated folders before we start our loop through time.",
        "metadata": {}
      },
      {
        "cell_type": "code",
        "source": "data_access_component = DataAccessComponent()\nget_static_data(data_access_component=data_access_component, roi=roi,\n                start_time=start_time_as_string, stop_time=stop_time_as_string, \n          emulation_directory=emulators_directory, dem_directory=dem_directory, roi_grid=roi_grid)",
        "metadata": {},
        "execution_count": 15,
        "outputs": [
          {
            "name": "stdout",
            "output_type": "stream",
            "text": "INFO     data_access_component _read_data_stores: 269: Read data store aws_s2\nINFO     data_access_component _read_data_stores: 269: Read data store cams\nINFO     data_access_component _read_data_stores: 269: Read data store cams_tiff\nINFO     data_access_component _read_data_stores: 269: Read data store emulators\nINFO     data_access_component _read_data_stores: 269: Read data store wv_emulator\nINFO     data_access_component _read_data_stores: 269: Read data store aster_dem\nINFO     data_access_component _read_data_stores: 269: Read data store modis_mcd43a1\nINFO     data_access_component _read_data_stores: 269: Read data store S2L2\nINFO     data_access_component _read_data_stores: 269: Read data store MundiRest\nRetrieving emulators ...\nDEBUG    connectionpool _new_conn: 208: Starting new HTTP connection (1): www2.geog.ucl.ac.uk\nDEBUG    connectionpool _make_request: 396: http://www2.geog.ucl.ac.uk:80 \"GET /~ucfafyi/emus/ HTTP/1.1\" 200 7702\nRetrieving DEM ...\nDEBUG    connectionpool _new_conn: 208: Starting new HTTP connection (1): www2.geog.ucl.ac.uk\nDEBUG    connectionpool _make_request: 396: http://www2.geog.ucl.ac.uk:80 \"GET /~ucfafyi/eles/ HTTP/1.1\" 200 None\nDone retrieving static data\n"
          }
        ]
      },
      {
        "cell_type": "markdown",
        "source": "# 6. Infer variables\nNow we can actually infer the bio-physical variables. For this, we will step though the time grid that we have set up using the start_time, end_time and time_step parameters above.\nFor this stepping, we create a *cursor* that points to the beginning of the time period that we are currently deriving data for.\nWe also define two variables *previous_inference_state* and *updated_inference_state* which will save the state of the inference engine and will ensure that the inference engine can consider the results from its last run.\nThe variable *preprocess_only_region_of_interest* allows to preprocess only the portion of the S2 image that we will consider during the inference. If we process the whole image though, we can save it and use it in later runs.",
        "metadata": {}
      },
      {
        "cell_type": "markdown",
        "source": "Thes stepping of time works as follows: Dedicated directories are set up for the MODIS, CAMS, and S2 data. These data are retrieved and put into these directories. After that, pre-processing will take place either on the whole S2 image or on the region of interest. If it has been performed on the whole region, the result will be permanently saved. Next, priors will be derived for every variable and every day within the current period. Having gathered all these, the inference can finally begin. The state of the inference engine is saved and considered during the next iteration.",
        "metadata": {}
      },
      {
        "cell_type": "markdown",
        "source": "## 6.1 Single Run Test\nFirst for explanation, we will perform the inference over a single timestep. During this, we will show all the results to indicate the flow of the processings. ",
        "metadata": {}
      },
      {
        "cell_type": "markdown",
        "source": "### 6.1.1 setting up",
        "metadata": {}
      },
      {
        "cell_type": "code",
        "source": "cursor = start_time_as_datetime\nprevious_inference_state = None #'none'\nupdated_inference_state = 'none'\none_day_step = datetime.timedelta(days=1)\npreprocess_only_region_of_interest = True\n\ndate_as_string = datetime.datetime.strftime(cursor, '%Y-%m-%d')\nprint('Doing time step starting on {}'.format(date_as_string))",
        "metadata": {},
        "execution_count": 16,
        "outputs": [
          {
            "name": "stdout",
            "output_type": "stream",
            "text": "Doing time step starting on 2018-07-24\n"
          }
        ]
      },
      {
        "cell_type": "code",
        "source": "cursor += time_step_as_time_delta\ncursor -= one_day_step\nif cursor > stop_time_as_datetime:\n    cursor = stop_time_as_datetime\nnext_date_as_string = datetime.datetime.strftime(cursor, '%Y-%m-%d')\ncursor += one_day_step\ncursor_as_string = datetime.datetime.strftime(cursor, '%Y-%m-%d')\n\nmodis_directory_for_date = '{}/{}'.format(modis_directory, date_as_string)\ncams_directory_for_date = '{}/{}'.format(cams_directory, date_as_string)\ns2_l1c_directory_for_date = '{}/{}'.format(s2_l1c_directory, date_as_string)\nsdrs_directory_for_date = '{}/{}'.format(sdrs_directory, date_as_string)\npriors_directory_for_date = '{}/{}/'.format(priors_directory, date_as_string)",
        "metadata": {},
        "execution_count": 17,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "source": "### 6.1.2 Preprocessing",
        "metadata": {}
      },
      {
        "cell_type": "code",
        "source": "get_dynamic_data(data_access_component, roi, roi_grid, date_as_string, next_date_as_string,\n                 modis_directory_for_date, cams_directory_for_date, s2_l1c_directory_for_date)\npreprocess(s2_l1c_directory_for_date, modis_directory_for_date, emulators_directory, cams_directory_for_date, \n           dem_directory, sdrs_directory_for_date, roi)\n",
        "metadata": {},
        "execution_count": null,
        "outputs": [
          {
            "name": "stdout",
            "output_type": "stream",
            "text": "Retrieving MODIS BRDF descriptors ...\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-08 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-09 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-10 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-11 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-12 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-13 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-14 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-15 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-16 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-17 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-18 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-19 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-20 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-21 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-22 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-23 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-24 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-25 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-26 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-27 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-28 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-29 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-30 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-07-31 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-08-01 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-08-02 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-08-03 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-08-04 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-08-05 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-08-06 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-08-07 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-08-08 00:00:00\nINFO     lpdaac_data_access _query_wrapped_meta_info_provider: 116: Found MCD43A1.006 data set for 2018-08-09 00:00:00\nRetrieving CAMS data ...\nDEBUG    connectionpool _new_conn: 208: Starting new HTTP connection (1): www2.geog.ucl.ac.uk\nDEBUG    connectionpool _make_request: 396: http://www2.geog.ucl.ac.uk:80 \"GET /~ucfafyi/cams/ HTTP/1.1\" 200 None\nRetrieving S2 L1C data ...\nDEBUG    connectionpool _new_conn: 824: Starting new HTTPS connection (1): roda.sentinel-hub.com\nDEBUG    connectionpool _make_request: 396: https://roda.sentinel-hub.com:443 \"GET /sentinel-s2-l1c/tiles/35/V/ME/2018/7/24/0/tileInfo.json HTTP/1.1\" 200 1493\nDEBUG    connectionpool _new_conn: 824: Starting new HTTPS connection (1): roda.sentinel-hub.com\nDEBUG    connectionpool _make_request: 396: https://roda.sentinel-hub.com:443 \"GET /sentinel-s2-l1c/tiles/35/V/ME/2018/7/24/1/tileInfo.json HTTP/1.1\" 404 0\nDEBUG    connectionpool _new_conn: 824: Starting new HTTPS connection (1): roda.sentinel-hub.com\nDEBUG    connectionpool _make_request: 396: https://roda.sentinel-hub.com:443 \"GET /sentinel-s2-l1c/tiles/35/V/NE/2018/7/24/0/tileInfo.json HTTP/1.1\" 200 1588\nDEBUG    connectionpool _new_conn: 824: Starting new HTTPS connection (1): roda.sentinel-hub.com\nDEBUG    connectionpool _make_request: 396: https://roda.sentinel-hub.com:443 \"GET /sentinel-s2-l1c/tiles/35/V/NE/2018/7/24/1/tileInfo.json HTTP/1.1\" 404 0\nDone retrieving dynamic data\nStart pre-processing S2 L1 data from 7-24-2018\n"
          }
        ]
      },
      {
        "cell_type": "code",
        "source": "# visualize the atmospheric correction\nProductnr = 0\nPlot_SRDS(sdrs_directory)",
        "metadata": {},
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "code",
        "source": "Productnr = 1\nPlot_SRDS(sdrs_directory, Productnr)\n",
        "metadata": {},
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "source": "### 6.1.3  Get Priors\nYou can choose to have the system create it's own 'default' configuration file (using the parameter values, specified earlier). Alternatively, one can make user-specific requests. For example, One can alter the Priors that are going to be used in the inference. If you would like to use these more complex metod, in the actual inference you should provide a link to this user configuration file (or uncomment this line, if one prefers the default configuration). \n\nPlease be aware that new files have to be created in case of user-defined priors, which can take some time (depending on number of 'user-defined' priors, the size of the area and the duration of the time-period). ",
        "metadata": {}
      },
      {
        "cell_type": "code",
        "source": "# Setting up the default configuration (of flat priors)\nconfig_file = None\nuser_priors = {}\nuser_priors['lai'] = {'mu': 3.0, 'unc': 0.2}\n# user_priors['cab'] = {'mu': 70.0, 'unc': 70.*0.0001}\n# user_priors['car'] = {'mu': 0.8, 'unc': 0.8*0.01}\n# user_priors['cw'] = {'mu': 0.005, 'unc': 0.005*0.01}\n# user_priors['cdm'] = {'mu': 0.0035, 'unc': 0.0035*0.01}\n# user_priors['cb'] = {'mu': 0.01, 'unc': 0.01*0.01}\n# user_priors['n'] = {'mu': 1.6, 'unc': 1.6*0.01}\n# user_priors['ala'] = {'mu': 70., 'unc': 70*0.01}\n\n\n# link to user-configuration file. Comment out to create  default configuration file\n# config_file = '/home/test_user_16/config_user.yaml'\n# Please be aware that the working directory in the configuration file should be the same as the working directory \n# specified above\n",
        "metadata": {},
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "code",
        "source": "cursor = start_time_as_datetime\ndate_as_string = datetime.datetime.strftime(cursor, '%Y-%m-%d')\npriors_directory_for_date = '{}/{}/'.format(priors_directory, date_as_string)\n",
        "metadata": {},
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "code",
        "source": "if config_file is not None:\n    get_priors_from_config_file(date_as_string, next_date_as_string, priors_directory_for_date, variables, config_file)\nelse:\n    get_priors(working_dir, roi, date_as_string, next_date_as_string, time_step, priors_directory_for_date, variables, user_priors)\n    config_file = working_dir + '/config.yaml'\n    ",
        "metadata": {},
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "code",
        "source": "# visualize Priors\nPlot_PRIORS(roi_centroid,priors_directory_for_date,variables)",
        "metadata": {},
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "source": "### 6.1.4  Retrieval",
        "metadata": {}
      },
      {
        "cell_type": "code",
        "source": "infer_new(config_file, date_as_string, cursor_as_string, None, priors_directory_for_date, sdrs_directory_for_date, \n          updated_inference_state, biophys_output,variables, None, spatial_resolution, roi_grid,destination_grid)",
        "metadata": {},
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "source": "#### 6.1.4.1  Visualize retrieval",
        "metadata": {}
      },
      {
        "cell_type": "code",
        "source": "import gdal \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport glob\n\nprint(biophys_output)\nvariables_subset = variables\ndate = '205'\n\nPlot_TRAITS(biophys_output,date,variables_subset)    ",
        "metadata": {},
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "source": "## 6.2 Inference of Full period",
        "metadata": {}
      },
      {
        "cell_type": "code",
        "source": "previous_inference_state = None\ncursor = start_time_as_datetime\ndate_as_string = datetime.datetime.strftime(cursor, '%Y-%m-%d')\npriors_directory_for_date = '{}/{}/'.format(priors_directory, date_as_string)\n\n\nwhile cursor <= stop_time_as_datetime:\n    # define parameters\n    date_as_string = datetime.datetime.strftime(cursor, '%Y-%m-%d')\n    print('Doing time step starting on {}'.format(date_as_string))\n    cursor += time_step_as_time_delta\n    cursor -= one_day_step\n    if cursor > stop_time_as_datetime:\n        cursor = stop_time_as_datetime\n    next_date_as_string = datetime.datetime.strftime(cursor, '%Y-%m-%d')\n    cursor += one_day_step\n    cursor_as_string = datetime.datetime.strftime(cursor, '%Y-%m-%d')\n\n    modis_directory_for_date = '{}/{}'.format(modis_directory, date_as_string)\n    cams_directory_for_date = '{}/{}'.format(cams_directory, date_as_string)\n    s2_l1c_directory_for_date = '{}/{}'.format(s2_l1c_directory, date_as_string)\n    sdrs_directory_for_date = '{}/{}'.format(sdrs_directory, date_as_string)\n    priors_directory_for_date = '{}/{}/'.format(priors_directory, date_as_string)\n    \n    # Preprocessing\n    get_dynamic_data(data_access_component, roi, roi_grid, date_as_string, next_date_as_string,\n                     modis_directory_for_date, cams_directory_for_date, s2_l1c_directory_for_date)\n    \n    if preprocess_only_region_of_interest:\n        preprocess(s2_l1c_directory_for_date, modis_directory_for_date, emulators_directory, cams_directory_for_date, \n                   dem_directory, sdrs_directory_for_date, roi)        \n    else:\n        preprocess(s2_l1c_directory_for_date, modis_directory_for_date, emulators_directory, cams_directory_for_date, \n                   dem_directory, sdrs_directory_for_date, 'none')\n        \n        put_data(data_access_component, sdrs_directory_for_date)\n        \n    \n    get_priors(working_dir, roi, date_as_string, next_date_as_string, time_step, priors_directory_for_date, variables)\n\n    updated_inference_state = '{}/{}'.format(hres_state_dir, date_as_string)\n    \n    infer_new(config_file, date_as_string, cursor_as_string, previous_inference_state, priors_directory_for_date, sdrs_directory_for_date, \n          updated_inference_state, biophys_output,variables, file_mask, spatial_resolution, roi_grid,destination_grid)\n    \n    previous_inference_state = updated_inference_state\nprint('DONE!')",
        "metadata": {},
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "source": "# 6 Results\nplease select the variable of interest",
        "metadata": {}
      },
      {
        "cell_type": "code",
        "source": "# parameter of interest to be plotted\nvariable_of_interest = 'lai'\n",
        "metadata": {},
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "source": "## 6.1 Read Data \nAll the variables are stored within the user directory. However for visualisation purposes we here only provide you to investigate a 'single variable of interest. Please find all the output files located at the following directory:",
        "metadata": {}
      },
      {
        "cell_type": "code",
        "source": "print(biophys_output)",
        "metadata": {},
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "code",
        "source": "import gdal\nimport numpy as np\n\ndata_files = set(glob.glob(biophys_output + '/*' + variable_of_interest + '*')) - \\\n            set(glob.glob(biophys_output + '/*unc*'))\nnum_of_data_sets = len(data_files)\n\nData = []\nprint('reading %s files into memory' % variable_of_interest)\nfor data_file in data_files:\n    data_set = gdal.Open(data_file)\n    data = data_set.ReadAsArray(0)\n    data[np.where(data<1e-8)]=np.NaN\n    data[np.where(data>0.99)]=np.NaN\n    \n    Data.append(data)    \nprint('Finished reading all %s files' % variable_of_interest)    \n\ndata_files",
        "metadata": {},
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "source": "## 6.2 Process Data\nWithin MULTIPLY we use transformation on some of the variables in order to (approximately) linearize the  radiative transfer model. In this sense, LAI within the state-vector is corrected for the exponential extinction of radiation that occurs witin the canopy. \n\nLAI = exp(LAI/-2.0)\n\nThis linearisation speeds-up the inference and also allows us to propagate the uncertainties better. However it comes at the expense that the output results of the inference are not in real-units (for LAI -> [m2/m2]). Instead they are in transformed space, ranging from 0.0 (for dense vegetation) - 1.0 (for bare soil). This is highlighted below. \n\nPlease also note that for LAI we use  relatively coarse resolution prior information. Therefore for retrievals over short time periods (particular locations with high cloud cover), this coarse resolution will be apparant.\n",
        "metadata": {}
      },
      {
        "cell_type": "markdown",
        "source": "LAI is not the only variable within the state-vector that are defined in transformed equivalents. The other transformed parameters are: leaf chlorophyll content (cab),leaf  water content (cw), leaf dry matter (cdm), and leaf average angle.\n\n\\begin{equation*}\n    cab_{real}    = (-100.0)* ln( cab_{t} )\n\\end{equation*}\n\n\\begin{equation*}\n    cw_{real}    = (-1/50.0)* log( data )\n\\end{equation*}\n\n\\begin{equation*}    \n    cdm_{real}    = (-1/100.0)* log( cdm_{t} )\n\\end{equation*}\n\n\\begin{equation*}    \n    LAI_{real}    = -2.0* log( LAI_{t} )\n\\end{equation*}\n\n\\begin{equation*}    \n    ALA_{real}    = 90.0 * ALA_{t}\n\\end{equation*}\n\nA Inverse Transformation module is provided within Tools.py to facilitate the retrieval of the actual values. ",
        "metadata": {}
      },
      {
        "cell_type": "code",
        "source": "Data_t = InvTransformation(variable_of_interest,Data)\n\nPlot_Transformation(Data,Data_t)",
        "metadata": {},
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "source": "## 6.3 Plot Temporal evolution",
        "metadata": {}
      },
      {
        "cell_type": "code",
        "source": "Plot_TRAIT_evolution(Data_t,variable_of_interest)    ",
        "metadata": {},
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "source": "# 7. Download the Data\nWe first need to compile all the biophysical parameters into a single zip file.",
        "metadata": {}
      },
      {
        "cell_type": "code",
        "source": "import os\nos.system('tar -cvf download/biophys_download.zip %s' % biophys_output)",
        "metadata": {},
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "code",
        "source": "from IPython.display import FileLink, FileLinks\nFileLinks('download')\n",
        "metadata": {},
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "markdown",
        "source": "# the Contributing team\n<img src=\"pics/Logos.png\">\n",
        "metadata": {}
      },
      {
        "cell_type": "code",
        "source": "",
        "metadata": {},
        "execution_count": null,
        "outputs": []
      },
      {
        "cell_type": "code",
        "source": "",
        "metadata": {},
        "execution_count": null,
        "outputs": []
      }
    ]
  }
}
